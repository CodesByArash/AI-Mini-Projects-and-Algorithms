{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0722f01b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "from mlp import MLP\n",
    "from metrics import mse_loss, precision, recall, f1_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8d26e55e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "data = load_iris()\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(data.data, data.target, test_size=0.3, random_state=42)\n",
    "\n",
    "# Convert the training and testing sets into input and labels\n",
    "train_input = X_train\n",
    "test_input = X_test\n",
    "\n",
    "# Initialize the StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit the scaler on the training data and transform both the training and testing data\n",
    "train_input = scaler.fit_transform(train_input)\n",
    "test_input = scaler.transform(test_input)\n",
    "\n",
    "\n",
    "# One-hot encode the target labels\n",
    "encoder = OneHotEncoder()\n",
    "train_labels = encoder.fit_transform(y_train.reshape(-1, 1)).toarray()\n",
    "test_labels = encoder.transform(y_test.reshape(-1, 1)).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = MLP(4, layer_dims = [64, 64, 3], activationfuncs=[ \"relu\", \"softmax\"], learning_rate=0.0005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mse loss :  0.21319867722053276\n",
      "mse loss :  0.17460981291663782\n",
      "mse loss :  0.14934478207032884\n",
      "mse loss :  0.1308970545515343\n",
      "mse loss :  0.11940023602040377\n",
      "mse loss :  0.08711969759487712\n",
      "mse loss :  0.07777267140822815\n",
      "mse loss :  0.07623473849871427\n",
      "mse loss :  0.07616775342305751\n",
      "mse loss :  0.07614110800263982\n",
      "mse loss :  0.07612130585440642\n",
      "mse loss :  0.0761018248230622\n",
      "mse loss :  0.07607956126862464\n",
      "mse loss :  0.07605148616617716\n",
      "mse loss :  0.07601332139319668\n",
      "mse loss :  0.07595807182865248\n",
      "mse loss :  0.07587451691515014\n",
      "mse loss :  0.0757485482512086\n",
      "mse loss :  0.07557515566751546\n",
      "mse loss :  0.07537515610605484\n",
      "mse loss :  0.07518768294481398\n",
      "mse loss :  0.07504010714476887\n",
      "mse loss :  0.07493091971080813\n",
      "mse loss :  0.07483154581296955\n",
      "mse loss :  0.0746968544973118\n",
      "mse loss :  0.07439326265998872\n",
      "mse loss :  0.06606949383875128\n",
      "mse loss :  0.057437866727489725\n",
      "mse loss :  0.032868251282596114\n",
      "mse loss :  0.03211917691277429\n",
      "mse loss :  0.03197126243146354\n",
      "mse loss :  0.03190845225294301\n",
      "mse loss :  0.031873542132686054\n",
      "mse loss :  0.031851280048678196\n",
      "mse loss :  0.031835816991549755\n",
      "mse loss :  0.031824437209496334\n",
      "mse loss :  0.0318157116355818\n",
      "mse loss :  0.03180880492537941\n",
      "mse loss :  0.03180320067738789\n",
      "mse loss :  0.031798555351796015\n",
      "mse loss :  0.03179464526406864\n",
      "mse loss :  0.03179130961647943\n",
      "mse loss :  0.03178843026493423\n",
      "mse loss :  0.03178591942145981\n",
      "mse loss :  0.03178371093553427\n",
      "mse loss :  0.03178175248357675\n",
      "mse loss :  0.031780004081948796\n",
      "mse loss :  0.031778433582364575\n",
      "mse loss :  0.03177701507293317\n",
      "mse loss :  0.03177572744507325\n",
      "mse loss :  0.03177455299635197\n",
      "mse loss :  0.031773477816296844\n",
      "mse loss :  0.03177248988016106\n",
      "mse loss :  0.03177157865440312\n",
      "mse loss :  0.03177073556410037\n",
      "mse loss :  0.03176995289230628\n",
      "mse loss :  0.03176922494901383\n",
      "mse loss :  0.03176854582296704\n",
      "mse loss :  0.03176791052548863\n",
      "mse loss :  0.03176731537606642\n",
      "mse loss :  0.03176675643269594\n",
      "mse loss :  0.03176623018865732\n",
      "mse loss :  0.03176573447356416\n",
      "mse loss :  0.03176526603553792\n",
      "mse loss :  0.031764823212797916\n",
      "mse loss :  0.03176440346052067\n",
      "mse loss :  0.03176400547332657\n",
      "mse loss :  0.031763627188126614\n",
      "mse loss :  0.03176326746600223\n",
      "mse loss :  0.03176292471662739\n",
      "mse loss :  0.03176259786223096\n",
      "mse loss :  0.03176228596370404\n",
      "mse loss :  0.03176198775909329\n",
      "mse loss :  0.03176170245444377\n",
      "mse loss :  0.03176142937781959\n",
      "mse loss :  0.031761167512076686\n",
      "mse loss :  0.031760916283567996\n",
      "mse loss :  0.0317606750335325\n",
      "mse loss :  0.031760443258749584\n",
      "mse loss :  0.031760220349400736\n",
      "mse loss :  0.03176000574848538\n",
      "mse loss :  0.03175979904460741\n",
      "mse loss :  0.03175959980668255\n",
      "mse loss :  0.03175940763473397\n",
      "mse loss :  0.031759222128481146\n",
      "mse loss :  0.03175904301046315\n",
      "mse loss :  0.03175886991351444\n",
      "mse loss :  0.03175870253734698\n",
      "mse loss :  0.031758540601147627\n",
      "mse loss :  0.03175838384227128\n",
      "mse loss :  0.03175823201478846\n",
      "mse loss :  0.03175808488838979\n",
      "mse loss :  0.031757942286662955\n",
      "mse loss :  0.03175780394094279\n",
      "mse loss :  0.03175766968773525\n",
      "mse loss :  0.0317575393470863\n",
      "mse loss :  0.03175741274946527\n",
      "mse loss :  0.03175728973496835\n",
      "mse loss :  0.03175717009893193\n",
      "mse loss :  0.031757053818961885\n"
     ]
    }
   ],
   "source": [
    "epoch=1000\n",
    "batch_size = 1\n",
    "for i in range(epoch):\n",
    "    outs = []\n",
    "    for j in range(0,len(train_input), batch_size):\n",
    "        x = train_input[j:j+batch_size]\n",
    "        y = train_labels[j:j+batch_size]\n",
    "        out = mlp(x.T)\n",
    "        mlp.backpropagation(y.T, out)\n",
    "        outs.append(out)\n",
    "    if i%10 == 0:\n",
    "        print(\"mse loss : \", mse_loss(train_labels, np.array(outs).reshape([105,3])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f199ffd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12 0 arash\n",
      "12 0 arash\n",
      "[[19  0  0]\n",
      " [ 0 12  1]\n",
      " [ 0  0 13]] :confusion_matrix\n",
      "1.0 :precision\n",
      "1.0 :recall\n",
      "1.0 :f1_score\n"
     ]
    }
   ],
   "source": [
    "outs= []\n",
    "for input, label in zip(test_input, y_test):\n",
    "    out = mlp(input.reshape(4,1))\n",
    "    outs.append(np.argmax(out))\n",
    "\n",
    "confusion_matrix = confusion_matrix(y_test,outs)\n",
    "precision        = precision(confusion_matrix)\n",
    "recall           = recall(confusion_matrix)\n",
    "f1_score         = f1_score(confusion_matrix)\n",
    "print(confusion_matrix,\":confusion_matrix\")\n",
    "print(precision,\":precision\")\n",
    "print(recall,\":recall\")\n",
    "print(f1_score,\":f1_score\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd4acd6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(repr(mlp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "59894727",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(142, 3)\n",
      "mse loss :  0.23055273508753207\n",
      "mse loss :  0.22200764630136868\n",
      "mse loss :  0.2158174880009909\n",
      "mse loss :  0.21188971762936137\n",
      "mse loss :  0.20959638888890297\n",
      "mse loss :  0.20817012769486504\n",
      "mse loss :  0.20711280812443603\n",
      "mse loss :  0.2061849264087133\n",
      "mse loss :  0.2052863884236396\n",
      "mse loss :  0.20437646338107487\n",
      "mse loss :  0.2034368928611217\n",
      "mse loss :  0.2024573226314541\n",
      "mse loss :  0.20142992448067673\n",
      "mse loss :  0.20034744060915813\n",
      "mse loss :  0.19920244029327533\n",
      "mse loss :  0.1979869936057387\n",
      "mse loss :  0.19669248755681598\n",
      "mse loss :  0.1953094961499617\n",
      "mse loss :  0.19382768217031868\n",
      "mse loss :  0.19223573156082013\n",
      "mse loss :  0.1905213282547835\n",
      "mse loss :  0.18867118130285104\n",
      "mse loss :  0.18667112780877496\n",
      "mse loss :  0.1845063626873139\n",
      "mse loss :  0.18216188716608062\n",
      "mse loss :  0.1796233036514625\n",
      "mse loss :  0.17687809038815783\n",
      "mse loss :  0.17391745609864556\n",
      "mse loss :  0.17073879702021127\n",
      "mse loss :  0.16734858860242421\n",
      "mse loss :  0.1637651244004797\n",
      "mse loss :  0.16001993749907134\n",
      "mse loss :  0.15615649444533905\n",
      "mse loss :  0.15222547400933742\n",
      "mse loss :  0.14827761817829335\n",
      "mse loss :  0.14435664493187522\n",
      "mse loss :  0.14049470411580553\n",
      "mse loss :  0.13671132867497157\n",
      "mse loss :  0.13301513894321623\n",
      "mse loss :  0.1294068611397374\n",
      "mse loss :  0.12588252638093328\n",
      "mse loss :  0.12243634672309249\n",
      "mse loss :  0.11906316743333865\n",
      "mse loss :  0.11576039993925301\n",
      "mse loss :  0.11252905569611084\n",
      "mse loss :  0.10937328548956286\n",
      "mse loss :  0.10629822637427722\n",
      "mse loss :  0.10330705718996926\n",
      "mse loss :  0.10039892439727384\n",
      "mse loss :  0.09756874359277531\n",
      "mse loss :  0.0948084644304587\n",
      "mse loss :  0.09210870709227914\n",
      "mse loss :  0.0894600089004299\n",
      "mse loss :  0.0868535054380556\n",
      "mse loss :  0.0842811971804152\n",
      "mse loss :  0.08173601597880503\n",
      "mse loss :  0.07921185800124327\n",
      "mse loss :  0.07670368666592453\n",
      "mse loss :  0.07420775825070558\n",
      "mse loss :  0.07172198146633774\n",
      "mse loss :  0.0692463816172927\n",
      "mse loss :  0.06678359526401272\n",
      "mse loss :  0.06433927604513051\n",
      "mse loss :  0.061922260173566564\n",
      "mse loss :  0.059544342327934346\n",
      "mse loss :  0.057219568458015735\n",
      "mse loss :  0.05496306420707226\n",
      "mse loss :  0.0527895618286606\n",
      "mse loss :  0.0507119150442132\n",
      "mse loss :  0.04873994224867158\n",
      "mse loss :  0.04687987367765573\n",
      "mse loss :  0.04513449991836605\n",
      "mse loss :  0.043503879778601415\n",
      "mse loss :  0.041986254864499216\n",
      "mse loss :  0.040578747187306155\n",
      "mse loss :  0.03927757671010472\n",
      "mse loss :  0.0380778845459811\n",
      "mse loss :  0.03697352746293695\n",
      "mse loss :  0.035957165820030175\n",
      "mse loss :  0.03502067694698671\n",
      "mse loss :  0.034155699727822175\n",
      "mse loss :  0.033354105526857254\n",
      "mse loss :  0.03260830138832212\n",
      "mse loss :  0.03191137079948629\n",
      "mse loss :  0.03125710005909295\n",
      "mse loss :  0.030639940712485634\n",
      "mse loss :  0.030054945675685184\n",
      "mse loss :  0.02949770256401696\n",
      "mse loss :  0.028964276701088015\n",
      "mse loss :  0.028451168248632502\n",
      "mse loss :  0.027955281711323828\n",
      "mse loss :  0.027473900961769316\n",
      "mse loss :  0.027004659123166835\n",
      "mse loss :  0.026545491419906135\n",
      "mse loss :  0.026094562103557845\n",
      "mse loss :  0.02565016431935241\n",
      "mse loss :  0.025210602337118745\n",
      "mse loss :  0.024774074723374315\n",
      "mse loss :  0.024338580818275776\n",
      "mse loss :  0.023901870601818687\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_wine\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the wine dataset\n",
    "data = load_wine()\n",
    "X = data.data   # Features (inputs)\n",
    "y = data.target # Target (outputs)\n",
    "\n",
    "# Split the dataset into a training set and a test set\n",
    "# test_size=0.2 means 20% of data is used for testing and 80% for training\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Now, X_train and y_train are the training inputs and outputs,\n",
    "# and X_test and y_test are the testing inputs and outputs.\n",
    "train_input = X_train\n",
    "test_input = X_test\n",
    "\n",
    "# Initialize the StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit the scaler on the training data and transform both the training and testing data\n",
    "train_input = scaler.fit_transform(train_input)\n",
    "test_input = scaler.transform(test_input)\n",
    "\n",
    "# One-hot encode the target labels\n",
    "encoder = OneHotEncoder()\n",
    "train_labels = encoder.fit_transform(y_train.reshape(-1, 1)).toarray()\n",
    "test_labels = encoder.transform(y_test.reshape(-1, 1)).toarray()\n",
    "\n",
    "print(train_labels.shape)\n",
    "\n",
    "epoch=1000\n",
    "\n",
    "# mlp = MLP(13, layer_dims = [128, 64, 3], activationfuncs=[\"softmax\", \"softmax\"], learning_rate=0.05)\n",
    "mlp = MLP(13, layer_dims = [128, 64, 3], activationfuncs=[\"softmax\", \"softmax\"], learning_rate=0.005)\n",
    "\n",
    "\n",
    "for i in range(epoch):\n",
    "    outs = []\n",
    "    for x, y in zip(train_input, train_labels):\n",
    "        out = mlp(x.reshape(13,1))\n",
    "        mlp.backpropagation(y.reshape(3,1), out)\n",
    "        outs.append(out)\n",
    "    if i%10 == 0:\n",
    "        print(\"mse loss : \", mse_loss(train_labels, np.array(outs).reshape([142,3])))\n",
    "\n",
    "outs= []\n",
    "for input, label in zip(train_input, y_train):\n",
    "    out = mlp(input.reshape(13,1))\n",
    "    # print(out)\n",
    "    outs.append(np.argmax(out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a2dfcc75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, -1,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0], dtype=int64)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outs= []\n",
    "for input, label in zip(test_input, y_test):\n",
    "    out = mlp(input.reshape(13,1))\n",
    "    # print(out)\n",
    "    outs.append(np.argmax(out))\n",
    "\n",
    "\n",
    "y_test-outs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "4640b5c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_normalization(batch, gamma, beta, epsilon=1e-5):\n",
    "    # Calculate mean and variance\n",
    "    mean = np.mean(batch, axis=0)\n",
    "    variance = np.var(batch, axis=0)\n",
    "    \n",
    "    # Normalize\n",
    "    batch_normalized = (batch - mean) / np.sqrt(variance + epsilon)\n",
    "    \n",
    "    # Scale and shift\n",
    "    batch_scaled_and_shifted = gamma * batch_normalized + beta\n",
    "    \n",
    "    return batch_scaled_and_shifted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74fc00f8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "13cfaab314e57165490be6244dd780a39caf64916f8b5ba446816839af7ad0cb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
